pout=sapply(rateout_original, sum)
pnull= sum(pout*m.eq.x/m.eq) - pout*m.eq.x/m.eq
r_nnn=max.person*pout+pnull
nnn = max(ceiling(r_nnn))
nnn = min(nnn,1000)
pout=pout/nnn
pnull=pnull/nnn
ratein=lapply(1:length(ratein_original), function(n) ratein_original[[n]]/nnn)
rateout=lapply(1:length(rateout_original), function(n) rateout_original[[n]]/nnn)
#print("--------------------------------nnn---------")
#print(nnn)
mini_timesteps[i] = nnn
if(nnn>1) new.t=c(new.t,i+1:(nnn-1)/nnn)
cv = sapply(1:length(locations), function(n) {
array(0,dim=length(action_list[[n]])-1)
})
for (k in 1:nnn){
t1 = i+(k-1)/nnn; t2 = i+k/nnn;
lb2=getSlice(lb,t2);
if(k!=nnn) {
tran=transition_forward_fra(la1,lb2,ratein, locin, rateout, locout, pout, pnull)
la2=tran$la2_tilde
if(length(attr(la,'t'))==length.la){la = alloc(la); length.la = length(la)}
if(min(abs(t2-attr(la,'t')))<1e-6) {
idx = which.min(abs(t2-attr(la,'t')))
} else {
attr(la,'t') = c(attr(la,'t'),t2);
idx = length(attr(la,'t'))
}
la[[idx]] = la2
} else {
tran=transition_forward_fra(la1,lb2,ratein, locin, rateout, locout, pout, pnull)
la2=tran$la2_tilde
la[[i+1]]=la2
}
exp_events = tran$exp_event
cv_t = sapply(1:length(locations), function(n) {
tmp_vector = exp_events[[n]]/sum(la1[[n]]*lb1[[n]]*0:max.person[n])
tmp_vector[is.na(tmp_vector)] = 0
tmp_vector
})
cv = Map('+',cv,cv_t)
la1=la2
lb1=lb2
} # k
exp_events_list[[i+1]]=cv
}
rate_slice[[3]]
theta = array(1e-1,dim=tot_theta)
theta[learnable] = action
rate_slice = theta2policy(theta)
theta_moveout = theta_g(max.person)
lg=sapply(1:length(locations),function(n) {
gamma=la1[[n]]*lb1[[n]]
gamma=gamma/sum(gamma)
sum(gamma* (0:(length(gamma)-1)))
})
for(n in c(4:21,23:25)) {
rate_slice[[n]][-1][] = theta_moveout[[n]][trunc(lg[n])+1]
rate_slice[[n]][1] = 1-sum(rate_slice[[n]][-1])
#print(rate_slice[[n]])
}
n = 22
rate_slice[[n]][3] = theta_moveout[[n]][trunc(lg[n])+1]
rate_slice[[n]][2] = 1e-8
rate_slice[[n]][1] = 1-sum(rate_slice[[n]][-1])
n = 3
length(rate_slice[[3]])
rate_slice[[n]][length(rate_slice[[n]])] = theta_moveout[[n]][trunc(lg[n])+1]
length(rate_slice[[3]])
rate_slice[[n]][-c(1,length(rate_slice[[n]]))] = 1e-8
length(rate_slice[[3]])
rate_slice[[n]][1] = 1-sum(rate_slice[[n]][-1])
length(rate_slice[[3]])
theta = array(1e-1,dim=tot_theta)
theta[learnable] = action
rate_slice = theta2policy(theta)
theta_moveout = theta_g(max.person)
lg=sapply(1:length(locations),function(n) {
gamma=la1[[n]]*lb1[[n]]
gamma=gamma/sum(gamma)
sum(gamma* (0:(length(gamma)-1)))
})
for(n in c(4:21,23:25)) {
rate_slice[[n]][-1][] = theta_moveout[[n]][trunc(lg[n])+1]
rate_slice[[n]][1] = 1-sum(rate_slice[[n]][-1])
#print(rate_slice[[n]])
}
n = 22
rate_slice[[22]]
action2rate <- function(action,max.person,la1,lb1) {
theta = array(1e-1,dim=tot_theta)
theta[learnable] = action
rate_slice = theta2policy(theta)
theta_moveout = theta_g(max.person)
lg=sapply(1:length(locations),function(n) {
gamma=la1[[n]]*lb1[[n]]
gamma=gamma/sum(gamma)
sum(gamma* (0:(length(gamma)-1)))
})
for(n in c(4:21,23:25)) {
rate_slice[[n]][-1][] = theta_moveout[[n]][trunc(lg[n])+1]
rate_slice[[n]][1] = 1-sum(rate_slice[[n]][-1])
#print(rate_slice[[n]])
}
n = 22
rate_slice[[n]][3] = theta_moveout[[n]][trunc(lg[n])+1]
rate_slice[[n]][2] = 1e-8
rate_slice[[n]][1] = 1-sum(rate_slice[[n]][-1])
n = 3
rate_slice[[n]][length(rate_slice[[n]])] = theta_moveout[[n]][trunc(lg[n])+1]
rate_slice[[n]][-c(1,length(rate_slice[[n]]))] = 1e-8
rate_slice[[n]][1] = 1-sum(rate_slice[[n]][-1])
rate_slice
}
rate_slice = action2rate(action,max.person,la1,lb1)
length(rate_slice[[3]])
length(rate_slice[[22]])
rate = rate_in_out_f(rate_slice,action_list)
ratein_original=rate$ratein
rateout_original=rate$rateout
m.eq=numeric(length = length(locations))
m.eq=sapply(1:length(locations),function(n) sum(la1[[n]]*lb2[[n]]))
m.eq[m.eq==0]=1e-20
m.eq.x=numeric(length = length(locations))
m.eq.x=sapply(1:length(locations),function(n) sum(0:max.person[n]*la1[[n]]*lb2[[n]]))
pout=sapply(rateout_original, sum)
pnull= sum(pout*m.eq.x/m.eq) - pout*m.eq.x/m.eq
r_nnn=max.person*pout+pnull
nnn = max(ceiling(r_nnn))
nnn = min(nnn,1000)
pout=pout/nnn
pnull=pnull/nnn
ratein=lapply(1:length(ratein_original), function(n) ratein_original[[n]]/nnn)
rateout=lapply(1:length(rateout_original), function(n) rateout_original[[n]]/nnn)
#print("--------------------------------nnn---------")
#print(nnn)
mini_timesteps[i] = nnn
if(nnn>1) new.t=c(new.t,i+1:(nnn-1)/nnn)
cv = sapply(1:length(locations), function(n) {
array(0,dim=length(action_list[[n]])-1)
})
tran=transition_forward_fra(la1,lb2,ratein, locin, rateout, locout, pout, pnull)
for(i in start.time:(end.time-1)){
la1=la[[i]]
lb1=lb[[i]]
lb2=lb[[i+1]]
lg=matrix(c(sapply(input_state,function(n) {
gamma=la1[[n]]*lb1[[n]]
gamma=gamma/sum(gamma)
sum(gamma* (0:(length(gamma)-1)))
}),i),nrow = 1)
lg = lg/state_scale
action = policy_network %>% predict(lg)
action = action/action_scale
cv_old[i-start.time+1,] = action
rate_slice = action2rate(action,max.person,la1,lb1)
rate = rate_in_out_f(rate_slice,action_list)
ratein_original=rate$ratein
rateout_original=rate$rateout
m.eq=numeric(length = length(locations))
m.eq=sapply(1:length(locations),function(n) sum(la1[[n]]*lb2[[n]]))
m.eq[m.eq==0]=1e-20
m.eq.x=numeric(length = length(locations))
m.eq.x=sapply(1:length(locations),function(n) sum(0:max.person[n]*la1[[n]]*lb2[[n]]))
pout=sapply(rateout_original, sum)
pnull= sum(pout*m.eq.x/m.eq) - pout*m.eq.x/m.eq
r_nnn=max.person*pout+pnull
nnn = max(ceiling(r_nnn))
nnn = min(nnn,1000)
pout=pout/nnn
pnull=pnull/nnn
ratein=lapply(1:length(ratein_original), function(n) ratein_original[[n]]/nnn)
rateout=lapply(1:length(rateout_original), function(n) rateout_original[[n]]/nnn)
#print("--------------------------------nnn---------")
#print(nnn)
mini_timesteps[i] = nnn
if(nnn>1) new.t=c(new.t,i+1:(nnn-1)/nnn)
cv = sapply(1:length(locations), function(n) {
array(0,dim=length(action_list[[n]])-1)
})
for (k in 1:nnn){
t1 = i+(k-1)/nnn; t2 = i+k/nnn;
lb2=getSlice(lb,t2);
if(k!=nnn) {
tran=transition_forward_fra(la1,lb2,ratein, locin, rateout, locout, pout, pnull)
la2=tran$la2_tilde
if(length(attr(la,'t'))==length.la){la = alloc(la); length.la = length(la)}
if(min(abs(t2-attr(la,'t')))<1e-6) {
idx = which.min(abs(t2-attr(la,'t')))
} else {
attr(la,'t') = c(attr(la,'t'),t2);
idx = length(attr(la,'t'))
}
la[[idx]] = la2
} else {
tran=transition_forward_fra(la1,lb2,ratein, locin, rateout, locout, pout, pnull)
la2=tran$la2_tilde
la[[i+1]]=la2
}
exp_events = tran$exp_event
cv_t = sapply(1:length(locations), function(n) {
tmp_vector = exp_events[[n]]/sum(la1[[n]]*lb1[[n]]*0:max.person[n])
tmp_vector[is.na(tmp_vector)] = 0
tmp_vector
})
cv = Map('+',cv,cv_t)
la1=la2
lb1=lb2
} # k
exp_events_list[[i+1]]=cv
}
source('vi-functions.R')
exp_events_list = list()
mini_timesteps = array(0,dim=1441)
new.t = c()
length.la = length(la)
length(exp_events_list) = 1441
cv_old = array(0,dim=c((end.time-start.time),length(learnable)))
for(i in start.time:(end.time-1)){
la1=la[[i]]
lb1=lb[[i]]
lb2=lb[[i+1]]
lg=matrix(c(sapply(1:2,function(n) {
gamma=la1[[n]]*lb1[[n]]
gamma=gamma/sum(gamma)
sum(gamma* (0:(length(gamma)-1)))
}),i),nrow = 1)
lg = lg/state_scale
action = policy_network %>% predict(lg)
action = action/action_scale
cv_old[i-start.time+1,] = action
rate_slice = action2rate(action,max.person,la1,lb1)
rate = rate_in_out_f(rate_slice,action_list)
ratein_original=rate$ratein
rateout_original=rate$rateout
m.eq=numeric(length = length(locations))
m.eq=sapply(1:length(locations),function(n) sum(la1[[n]]*lb2[[n]]))
m.eq[m.eq==0]=1e-20
m.eq.x=numeric(length = length(locations))
m.eq.x=sapply(1:length(locations),function(n) sum(0:max.person[n]*la1[[n]]*lb2[[n]]))
pout=sapply(rateout_original, sum)
pnull= sum(pout*m.eq.x/m.eq) - pout*m.eq.x/m.eq
r_nnn=max.person*pout+pnull
nnn = max(ceiling(r_nnn))
pout=pout/nnn
pnull=pnull/nnn
ratein=lapply(1:length(ratein_original), function(n) ratein_original[[n]]/nnn)
rateout=lapply(1:length(rateout_original), function(n) rateout_original[[n]]/nnn)
#print("--------------------------------nnn---------")
#print(nnn)
mini_timesteps[i] = nnn
if(nnn>1) new.t=c(new.t,i+1:(nnn-1)/nnn)
cv = sapply(1:length(locations), function(n) {
array(0,dim=length(action_list[[n]])-1)
})
for (k in 1:nnn){
t1 = i+(k-1)/nnn; t2 = i+k/nnn;
lb2=getSlice(lb,t2);
if(k!=nnn) {
tran=transition_forward_fra(la1,lb2,ratein, locin, rateout, locout, pout, pnull)
la2=tran$la2_tilde
if(length(attr(la,'t'))==length.la){la = alloc(la); length.la = length(la)}
if(min(abs(t2-attr(la,'t')))<1e-6) {
idx = which.min(abs(t2-attr(la,'t')))
} else {
attr(la,'t') = c(attr(la,'t'),t2);
idx = length(attr(la,'t'))
}
la[[idx]] = la2
} else {
tran=transition_forward_fra(la1,lb2,ratein, locin, rateout, locout, pout, pnull)
la2=tran$la2_tilde
la[[i+1]]=la2
}
exp_events = tran$exp_event
cv_t = sapply(1:length(locations), function(n) {
tmp_vector = exp_events[[n]]/sum(la1[[n]]*lb1[[n]]*0:max.person[n])
tmp_vector[is.na(tmp_vector)] = 0
tmp_vector
})
cv = Map('+',cv,cv_t)
la1=la2
lb1=lb2
} # k
exp_events_list[[i+1]]=cv
}
input_state
for(i in start.time:(end.time-1)){
la1=la[[i]]
lb1=lb[[i]]
lb2=lb[[i+1]]
lg=matrix(c(sapply(input_state,function(n) {
gamma=la1[[n]]*lb1[[n]]
gamma=gamma/sum(gamma)
sum(gamma* (0:(length(gamma)-1)))
}),i),nrow = 1)
lg = lg/state_scale
action = policy_network %>% predict(lg)
action = action/action_scale
cv_old[i-start.time+1,] = action
rate_slice = action2rate(action,max.person,la1,lb1)
rate = rate_in_out_f(rate_slice,action_list)
ratein_original=rate$ratein
rateout_original=rate$rateout
m.eq=numeric(length = length(locations))
m.eq=sapply(1:length(locations),function(n) sum(la1[[n]]*lb2[[n]]))
m.eq[m.eq==0]=1e-20
m.eq.x=numeric(length = length(locations))
m.eq.x=sapply(1:length(locations),function(n) sum(0:max.person[n]*la1[[n]]*lb2[[n]]))
pout=sapply(rateout_original, sum)
pnull= sum(pout*m.eq.x/m.eq) - pout*m.eq.x/m.eq
r_nnn=max.person*pout+pnull
nnn = max(ceiling(r_nnn))
pout=pout/nnn
pnull=pnull/nnn
ratein=lapply(1:length(ratein_original), function(n) ratein_original[[n]]/nnn)
rateout=lapply(1:length(rateout_original), function(n) rateout_original[[n]]/nnn)
#print("--------------------------------nnn---------")
#print(nnn)
mini_timesteps[i] = nnn
if(nnn>1) new.t=c(new.t,i+1:(nnn-1)/nnn)
cv = sapply(1:length(locations), function(n) {
array(0,dim=length(action_list[[n]])-1)
})
for (k in 1:nnn){
t1 = i+(k-1)/nnn; t2 = i+k/nnn;
lb2=getSlice(lb,t2);
if(k!=nnn) {
tran=transition_forward_fra(la1,lb2,ratein, locin, rateout, locout, pout, pnull)
la2=tran$la2_tilde
if(length(attr(la,'t'))==length.la){la = alloc(la); length.la = length(la)}
if(min(abs(t2-attr(la,'t')))<1e-6) {
idx = which.min(abs(t2-attr(la,'t')))
} else {
attr(la,'t') = c(attr(la,'t'),t2);
idx = length(attr(la,'t'))
}
la[[idx]] = la2
} else {
tran=transition_forward_fra(la1,lb2,ratein, locin, rateout, locout, pout, pnull)
la2=tran$la2_tilde
la[[i+1]]=la2
}
exp_events = tran$exp_event
cv_t = sapply(1:length(locations), function(n) {
tmp_vector = exp_events[[n]]/sum(la1[[n]]*lb1[[n]]*0:max.person[n])
tmp_vector[is.na(tmp_vector)] = 0
tmp_vector
})
cv = Map('+',cv,cv_t)
la1=la2
lb1=lb2
} # k
exp_events_list[[i+1]]=cv
}
nnn
rm(list=ls())
source('0.MH_fb_functions.R')
if (TRUE) {
lr_history = list()
la_original = la
lb_original = lb
obs = loc.d
info_network$info_facility$begin[[1]] = 16
info_network$info_facility$begin[[2]] = 8
info_network$info_facility$end[[1]] = 8
info_network$info_facility$end[[2]] = 16
max.person = unlist(lapply(la_original[[1]],function(x) length(x)))-1
R_min = max.person[1]*(-18)
RLITER = 5
VIITER = 1
t0 = 1
t0.max = 1441
heavy_traffic = apply(loc.d,2,max)
names(heavy_traffic) = c(1:length(heavy_traffic))
heavy_traffic = rev(sort(heavy_traffic))
heavy_traffic = as.numeric(names(heavy_traffic[1:100]))
tmp_accum = sapply(1:length(action_list),function(n) {
length(action_list[[n]])-1
})
tmp_accum = cumsum(tmp_accum)
learnable = c()
for(i in heavy_traffic) {
learnable = c(learnable,tmp_accum[i-1]+1:(length(action_list[[i]])-1) )
}
action_scale = 1e3
state_scale = 10
input_state = heavy_traffic
output_state = learnable
tot_theta = 89241
Xt_real = loc.d
train_x = cbind(Xt_real[1:1200,heavy_traffic],c(1:1200))
train_y = array(0,dim=c(nrow(train_x),length(learnable)))
for(t0 in c(1:1200)) {
idx = trunc(t0/360)+1
theta = unlist(lapply(PpolicyUpdate[[idx]],function(x) x[-1]))
train_y[t0,] = theta[learnable]
}
train_y = train_y*action_scale*0.9
train_x = train_x/state_scale
}
save.image("berlin.RData")
source('vi-functions.R')
# policy network
if (TRUE) {
# SynthTown
#policy_network = keras_model_sequential() %>% layer_dense(units=12, input_shape = c(length(input_state)+1), activation='relu') %>% layer_dense(units=24, input_shape = c(3), activation='relu') %>% layer_dense(units=length(learnable), activation='relu')%>% compile(optimizer = optimizer_adam(lr=.001),loss = 'mse')
# Berlin
policy_network = keras_model_sequential() %>% layer_dense(units=64, input_shape = c(length(input_state)+1), activation='relu') %>% layer_dense(units=64, input_shape = c(3), activation='relu') %>% layer_dense(units=length(learnable), activation='relu')%>% compile(optimizer = optimizer_adam(lr=.001),loss = 'mse')
# pretrain
history <- policy_network %>% fit(x=train_x, y=train_y, epochs=1000, batch_size=100,verbose=0)
print(summary(history$metrics[[1]]))
action = policy_network %>% predict(train_x)
plot(rowSums(train_y[,1:9]),type='l')
lines(rowSums(action[,1:9]),type='l',col='red')
lines(train_y[,10],type='l')
lines(action[,10],type='l',col='blue')
}
for(iter_r in 1:RLITER){
cat(sprintf("iter_r: %d\n",iter_r))
la_E = la_original
lb_E = lb_original
xt_est = array(0,dim=c(nrow(obs),ncol(obs)))
train_x = c()
train_y = c()
t0 = 1
start.time = 1
end.time = 1441
H = end.time - start.time
obs = loc.d
# E-step
#######################################################################################################
if (TRUE) {
cat(sprintf("E-step\n"))
aaa = forward2(start.time, end.time, la_E, lb_E, action_list, max.person,policy_network,learnable)
la_E=aaa$la
#estimate vehicle distribution
for(ii in 1:ncol(obs)){
xt_est[start.time:end.time,ii] = sapply(start.time:end.time, function(n){
gamma=la_E[[n]][[ii]]
gamma=gamma/sum(gamma)
sum(gamma* (0:(length(gamma)-1))) })
}
# compute rewards
lr_accum = 0
for(tt in start.time:(end.time-1)) {
lg_test = la_E[[tt]]
group_idx = 1
lr = RewardFunction(max.person, tt, locations, action_list, info_network)
lr = RewardNormalization(lr,R_min)
lr_l = unlist(sapply(c(1:length(locations)),function(n) sum(lr[[n]]*lg_test[[n]])))
lr_accum = lr_accum + sum(lr_l/length(locations)/(end.time-start.time))
}
lr_accum = log(lr_accum)
}
# M-step
#######################################################################################################
if (TRUE) {
cat(sprintf("M-step\n"))
for(iter_v in 1:VIITER) {
bbb = backward2(start.time, end.time, la_E, lb_E, action_list, max.person,policy_network,learnable)
lb_E=bbb$lb
aaa = forward2(start.time, end.time, la_E, lb_E, action_list, max.person,policy_network,learnable)
la_E=aaa$la
exp_events_list=aaa$exp_events_list
cv_old=aaa$cv_old
new.t=aaa$new.t
mini_timesteps=aaa$mini_timesteps
}
mstep = M_step(start.time, end.time, la_E, lb_E, theta, action_list, max.person, new.t, mini_timesteps,exp_events_list,policy_network,learnable,cv_old)
train_x = rbind(train_x,mstep$train_x)
train_y = rbind(train_y,mstep$train_y)
}
if (TRUE) {
train_y = train_y*action_scale
train_x = train_x/state_scale
history <- policy_network %>% fit(x=train_x, y=train_y, epochs=2000, batch_size=100,verbose=0)
print(summary(history$metrics[[1]]))
action = policy_network %>% predict(train_x)
plot(rowSums(train_y[,1:9]),type='l')
lines(rowSums(action[,1:9]),type='l',col='red')
lines(train_y[,10],type='l')
lines(action[,10],type='l',col='blue')
}
# plot
#######################################################################################################
if (TRUE) {
if (iter_r == 1) {
xt_baseline = xt_est
}
#plot vechile distribution
layout(matrix(1:ncol(obs),ncol=1), heights=pmax(apply(obs,2,max),apply(loc.d,2,max))+10)
par(mar=c(0,0,0,0),oma=c(5,2,0,1)+.1)
for(ii in 1:ncol(obs)){
plot(xt_baseline[,ii],type='l',col='black',xaxt='n',xlab='',ylab=colnames(obs)[ii],ylim = c(0,max(loc.d[,ii])+1))
lines(xt_est[,ii],col="red",lty=1)
if(ii==1) text(1300,10,paste(iter_r, " f lg" ),col = 'red',lwd=3)
abline(v=c(540,1020),col='gray')
}
axis(side=1)
# compute rewards
lr_history = lappend(lr_history,lr_accum)
cat(sprintf("System log expected future rewards at %d: %f\n",t0,lr_accum))
}
# save data
if (TRUE) {
xt_est_vi = xt_est
save(xt_est_vi,file='result/xt_est_vi.RData')
save(lr_history,file='result/lr_history.RData')
}
}
