{
    "collab_server" : "",
    "contents" : "\nsource('0.MH_fb_functions.R')\n\n# to be put in preparation\nif (TRUE) {\n  lr_history = list()\n  la_original = la\n  lb_original = lb\n  obs = loc.d\n  td_init = 231\n  td = c(232,480,720,960,1200,1440)-td_init\n  time_length = td\n  time_length = diff(time_length)\n  #time_length[2] = time_length[2] + 120\n  time_trunc = cut(1:1441,breaks = c(td[1:5],td[6]+1)-1)\n  levels(time_trunc) = 1:5\n  time_trunc = as.numeric(time_trunc)\n  info_network$info_facility$begin[[1]] = 16\n  info_network$info_facility$begin[[2]] = 8    \n  info_network$info_facility$end[[1]] = 8\n  info_network$info_facility$end[[2]] = 16\n  max.person = unlist(lapply(la_original[[1]],function(x) length(x)))-1\n  R_min = max(max.person)*(-18)\n  RLITER = 10\n  VIITER = 1\n  \n  theta_list = list()\n  theta_original = list()\n  t0 = 1\n  t0.max = 1441\n  while (t0 < 5) {\n    start.time = t0\n    \n    # to be changed: theta 8 or 43\n    # to be changed: initial theta, 0.9 -> 0.1\n    theta = PpolicyUpdate[[start.time]]\n    theta = lapply(theta,function(x) x[-1])\n    theta = unlist(theta)\n    \n    theta_list = lappend(theta_list,theta)\n    theta_original = lappend(theta_original,theta)\n    \n    trunc_idx = time_trunc[t0]\n    H = time_length[trunc_idx]\n    t0 = t0 + 1\n  }\n  theta_list = lappend(theta_list,theta)\n  theta_original = lappend(theta_original,theta)\n}\n\n# Main loop\n#######################################################################################################\nfor(iter_r in 1:RLITER){\n  cat(sprintf(\"iter_r: %d\\n\",iter_r))\n\n  la_E = la_original\n  lb_E = lb_original\n  lr_accum_list = list()\n  xt_est = array(0,dim=c(nrow(obs),ncol(obs)))\n  \n  t0 = 1\n  while (t0 < t0.max) {\n    # preprocessing\n    #######################################################################################################\n    if (TRUE) {\n      print(t0)\n      #trunc_idx = ceiling(t0/H)\n      trunc_idx = time_trunc[t0]\n      H = time_length[trunc_idx]\n      start.time = t0\n      end.time = t0+H\n      theta = theta_list[[trunc_idx]]\n      obs = loc.d\n    }\n    \n    # Policy Execution\n    #######################################################################################################\n    if (TRUE) {\n      aaa = forward2(start.time, end.time, la_E, lb_E, theta, action_list, max.person)\n      la_E=aaa$la\n      \n      #plot-test\n      for(ii in 1:ncol(obs)){\n        xt_est[start.time:end.time,ii] = sapply(start.time:end.time, function(n){ \n          #gamma=rowSums(la_E[[n]][[ii]])\n          gamma=la_E[[n]][[ii]]\n          gamma=gamma/sum(gamma)\n          sum(gamma* (0:(length(gamma)-1))) })\n      }\n      \n      # compute rewards\n      lr_accum = 0\n      for(tt in start.time:(end.time-1)) {\n        lg_test = la_E[[tt]]\n        lr = RewardFunction(max.person, tt, locations, action_list, info_network)\n        # log, shift reward to all positive\n        lr = RewardNormalization(lr,R_min)\n        lr_l = unlist(sapply(c(1:length(locations)),function(n) sum(lr[[n]]*lg_test[[n]])))\n        lr_accum = lr_accum + sum(lr_l/length(locations)/(end.time-start.time))\n      }\n      #cat(sprintf(\"System log expected future rewards at %d: %f\\n\",t0,lr_accum_list))\n      lr_accum_list = lappend(lr_accum_list,log(lr_accum))\n    }\n    \n    # Policy Maximization\n    #######################################################################################################\n    if (TRUE) {\n      cat(sprintf(\"M-step\\n\"))\n      for(iter_v in 1:VIITER) {\n        bbb = backward2(start.time, end.time, la_E, lb_E, theta, action_list, max.person)\n        lb_E=bbb$lb \n        aaa = forward2(start.time, end.time, la_E, lb_E, theta, action_list, max.person)\n        la_E=aaa$la\n      }\n      exp_events_list=aaa$exp_events_list\n      new.t=aaa$new.t\n      mini_timesteps=aaa$mini_timesteps\n      \n      mstep = M_step(start.time, end.time, la_E, lb_E, theta, action_list, max.person, new.t, mini_timesteps,exp_events_list)\n      \n      # line search\n      theta_cur = theta\n      theta_next = unlist(mstep$cv)\n      \n      heavy_traffic = apply(loc.d,2,max)\n      names(heavy_traffic) = c(1:length(heavy_traffic))\n      heavy_traffic = rev(sort(heavy_traffic))\n      heavy_traffic = as.numeric(names(heavy_traffic[1:100]))\n      tmp_accum = sapply(1:length(action_list),function(n) {\n        length(action_list[[n]])-1\n      })\n      tmp_accum = cumsum(tmp_accum)\n      \n      learnable = c()\n      for(i in heavy_traffic) {\n        learnable = c(learnable,tmp_accum[i-1]+1:(length(action_list[[i]])-1) )\n      }\n      \n      epsilon = 1\n      \n      if (FALSE) {\n        cur_value = 0\n        ls_array = c(1,10,35,100)\n        for(ls in 1:length(ls_array)) {\n          theta_tmp = theta\n          theta_tmp[learnable] = theta_tmp[learnable] + ls_array[ls]*(theta_next-theta_tmp)[learnable]\n          theta_tmp = pmax(theta_tmp,0)\n          aaa = forward2(start.time, end.time, la_E, lb_E, theta_tmp, action_list, max.person)\n          la_E=aaa$la\n          lr_accum = 0\n          for(tt in start.time:(end.time-1)) {\n            lg_test = la_E[[tt]]\n            group_idx = 1\n            lr = RewardFunction(max.person, tt, locations, action_list, info_network, group_idx)\n            lr = RewardNormalization(lr,R_min)\n            lr_l = unlist(sapply(c(1:length(locations)),function(n) sum(lr[[n]]*lg_test[[n]])))\n            lr_accum = lr_accum + sum(lr_l/length(locations)/(end.time-start.time))\n          }\n          next_value = log(lr_accum)\n          cat(sprintf(\"line search with epsilon %d: %f\\n\",ls_array[ls],next_value))\n          if (next_value <= cur_value) {\n            epsilon = ls_array[ls-1]\n            break\n          }\n          cur_value = next_value\n          epsilon = ls_array[ls]\n        } \n      }\n      \n      theta[learnable] = theta[learnable] + epsilon*(theta_next-theta)[learnable]\n      theta = pmax(theta,0)\n      #theta = pmin(theta,1)\n      theta_list[[trunc_idx]] = theta\n    }\n    \n    t0 = t0 + H\n  }\n  \n  # plot\n  #######################################################################################################\n  if (TRUE) {\n    if (iter_r == 1) {\n      xt_baseline = xt_est\n    }\n    \n    #plot-test\n    if (FALSE) {\n      layout(matrix(1:ncol(obs),ncol=1), heights=pmax(apply(obs,2,max),apply(loc.d,2,max))+10)\n      par(mar=c(0,0,0,0),oma=c(5,2,0,1)+.1)\n      for(ii in 1:ncol(obs)){\n        plot(xt_baseline[,ii],type='l',col='black',xaxt='n',xlab='',ylab=colnames(obs)[ii],ylim = c(0,max(loc.d[,ii])+1))\n        #lines(obs[,ii],type='l',lty=2,col='black',xaxt='n',xlab='',ylab=colnames(obs)[ii],ylim = c(0,max(obs[,ii])+1))\n        lines(xt_est[,ii],col=\"red\",lty=1)\n        if(ii==1) text(1300,10,paste(iter_r, \" f lg\" ),col = 'red',lwd=3)\n        abline(v=c(540,1020),col='gray')\n      }\n      axis(side=1) \n    }\n    \n    # compute rewards\n    lr_history = lappend(lr_history,lr_accum_list)\n    cat(sprintf(\"System log expected future rewards at %d: %f\\n\",t0,lr_accum_list))\n  }\n  \n  # save data\n  if (TRUE) {\n    save(theta_list,file='result/theta_list.RData')\n    xt_est_vi = xt_est\n    save(xt_est_vi,file='result/xt_est_vi.RData')\n    save(lr_history,file='result/lr_history.RData')\n  }\n}\n\n\n\n\n\n\n\n\n\n",
    "created" : 1526406103915.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "140|18|167|6|\n186|16|197|4|\n",
    "hash" : "707142975",
    "id" : "338B4280",
    "lastKnownWriteTime" : 1526422564,
    "last_content_update" : 1526422564236,
    "path" : "~/Documents/R/RL_VI_2018_NIPS/VI_RL8_a=v_berlin/main_dp.R",
    "project_path" : "main_dp.R",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}