{
    "collab_server" : "",
    "contents" : "\nlibrary(ggplot2)\nrequire(Hmisc)\nrequire(numDeriv)\nrequire(keras)\n\n# SynthTown\n#load(\"inference_1_synthtown.RData\")\n# berlin\nload(\"berlin.RData\")\n\ntheta2policy <- function(theta) {\n  \n  theta_list <- list()\n  tmp_accum = 0\n  for (tt in 1:length(action_list)) {\n    tmp_idx <- c((tmp_accum+1): (tmp_accum+length(action_list[[tt]])-1))\n    tmp_l <- theta[tmp_idx]\n    if (sum(tmp_l)>0.99999999) tmp_l = tmp_l/(sum(tmp_l)+1e-8)\n    theta_list[[tt]] <- c(1-sum(tmp_l),tmp_l)\n    tmp_accum <- tmp_accum + length(tmp_idx)\n  }\n  theta_list\n}\n\nla_init <- function(loc.d,locations,max.person,action_list,PpolicyUpdate,start.time,la) {\n  sliceempty=lapply(1:length(locations), function(m){\n    array(0,dim=max.person[m]+1)\n  })\n  start=sliceempty\n  for( i in 1:length(locations)) start[[i]][loc.d[start.time,i]+1]=1\n  \n  la[[start.time]]=start\n  return(la)\n}\n\nlappend <- function (lst, ...){\n  lst <- c(lst, list(...))\n  return(lst)\n}\n\nRewardNormalization <- function(lr,R_min) {\n  result = lapply(lr,function(x) x-R_min)\n  result[[1]] = result[[1]]-min(result[[1]])\n  result[[2]] = result[[2]]-min(result[[2]])\n  result\n}\n\ntheta_g<-function(max.person) {\n  offset_facility <- length(info_network$facility)\n  TimeMoveOut <- sapply(c(1:length(info_network$link)), function(n) {\n    list(pmax(c(0:max.person[n+length(info_network$facility)])/info_network$info_road$Roadload[n]*info_network$info_road$TimeMoveOut[n],info_network$info_road$TimeMoveOut[n]))\n  })\n  Pmoveout <- lapply(TimeMoveOut,function(x) 1/x)\n  Pfacility <- rep(list(1),length(info_network$facility))\n  for(i in c(1:length(info_network$facility))) {\n    Pfacility[[i]] <- array(1e-3,dim=max.person[info_network$facility[i]]+1)\n  }\n  \n  Ptransit = append(Pfacility,Pmoveout)\n  \n  Ptransit\n}\n\naction2rate <- function(action,max.person,la1,lb1) {\n  theta = array(1e-1,dim=tot_theta)\n  theta[learnable] = action\n  rate_slice = theta2policy(theta)\n  \n  theta_moveout = theta_g(max.person)\n  lg=sapply(1:length(locations),function(n) {\n    gamma=la1[[n]]*lb1[[n]]\n    gamma=gamma/sum(gamma)\n    sum(gamma* (0:(length(gamma)-1)))\n  })\n  for(n in c(4:21,23:25)) {\n    rate_slice[[n]][-1][] = theta_moveout[[n]][trunc(lg[n])+1]\n    rate_slice[[n]][1] = 1-sum(rate_slice[[n]][-1])\n  }\n  n = 22\n  rate_slice[[n]][3] = theta_moveout[[n]][trunc(lg[n])+1]\n  rate_slice[[n]][2] = 1e-8\n  rate_slice[[n]][1] = 1-sum(rate_slice[[n]][-1])\n  n = 3\n  rate_slice[[n]][length(rate_slice[[n]])] = theta_moveout[[n]][trunc(lg[n])+1]\n  rate_slice[[n]][-c(1,length(rate_slice[[n]]))] = 1e-8\n  rate_slice[[n]][1] = 1-sum(rate_slice[[n]][-1])\n  \n  rate_slice\n}\n\nEarlyLatePenalty <- function(t,goal_agents_cur,info_network) {\n  BETA_late.ar <- -18\n  BETA_early.dp <- -1\n  S_late.ar.q <- 0\n  S_early.dp.q <- 0\n  mid <- (info_network$info_facility$begin[[goal_agents_cur]] + info_network$info_facility$end[[goal_agents_cur]])/2\n  switch(goal_agents_cur,\n         '1'={\n           if (t/60 > mid) {\n             S_late.ar.q <- BETA_late.ar\n           } else {\n             S_early.dp.q <- BETA_early.dp\n           }\n         },\n         '2'={\n           if (t/60 < mid) {\n             S_late.ar.q <- BETA_late.ar\n           } else {\n             S_early.dp.q <- BETA_early.dp\n           }\n         })\n  \n  return(S_late.ar.q+S_early.dp.q)\n}\nRewardFunction<-function(max.person, cur_time, locations, action_list, info_network){\n  \n  BETA_dur <- 6 #staying in goal facility\n  BETA_trav.modeq = -6\n  t = cur_time\n  if ((t <= info_network$info_facility$begin[[2]]*60)||(t > info_network$info_facility$end[[2]]*60)) {\n    goal_agents_cur = 1\n  } else {\n    goal_agents_cur = 2\n  }\n  \n  # state\n  if (TRUE) {\n    S_plan <- list()\n    for(l in c(1:length(locations))) {\n      S_plan_l <- 0\n      S_trav.q = 0\n      S.dur.q = 0\n      S.ear.lat.penalty = 0\n      if(l %in% info_network$facility) {\n        if(l==goal_agents_cur) {\n          S.dur.q <- BETA_dur\n        } else {\n          S.ear.lat.penalty <- EarlyLatePenalty(t,goal_agents_cur,info_network)\n        }\n      } else {\n        S_trav.q <- BETA_trav.modeq\n        S.ear.lat.penalty <- EarlyLatePenalty(t,goal_agents_cur,info_network)\n      }\n      S_plan_l[] <- S_trav.q + S.dur.q + S.ear.lat.penalty\n      S_plan <- lappend(S_plan,S_plan_l)\n    }\n    \n    xt_reward = lapply(c(1:length(locations)),function(n) {\n      0:max.person[n]*S_plan[[n]]\n    })\n  }\n  \n  return(xt_reward)\n}\n\n# forward\n#######################################################################################################\n\nupd_forward<-function(v1,inc,out,pn,dec,len){\n  v2=v1*pn - 0:(len-1)*v1*out\n  v2[2:len]=v2[2:len] + v1[1:(len-1)]*inc\n  v2[1:(len-1)]=v2[1:(len-1)] + 1:(len-1)*v1[2:len]*dec\n  v2\n}\n\ntransition_forward_fra<-function(la1,lb2,ratein, locin, rateout, locout, pout, pnull){\n  m.inc=sapply(1:length(locations),function(n) sum( la1[[n]][1:max.person[n]] * lb2[[n]][2:(max.person[n]+1)] ) )\n  m.eq.x=sapply(1:length(locations),function(n) sum(0:max.person[n]*la1[[n]]*lb2[[n]]))\n  m.eq=sapply(1:length(locations),function(n) sum(la1[[n]]*lb2[[n]]))\n  m.eq[m.eq==0]=1e-20\n  m.dec=sapply(1:length(locations),function(n) sum( 1:max.person[n] * la1[[n]][2:(max.person[n]+1)] * lb2[[n]][1:max.person[n]] ))\n  \n  fra.eq=m.eq.x/m.eq\n  fra.inc=m.inc/m.eq\n  fra.dec=m.dec/m.eq\n  pinc=sapply(1:length(locations),function(n) sum(  ratein[[n]] * fra.dec[locin[[n]] ] )) \n  pdec= sapply(1:length(locations),function(n)  sum(  rateout[[n]] * fra.inc[locout[[n]]  ] ) )\n  \n  #for each link, calculate the prob of transition at all other links \n  tran=lapply(1:length(locations), function(n)  rateout[[n]] * fra.dec[n] * fra.inc[locout[[n]]  ] )\n  alltran=sum(unlist(tran))\n  trother=numeric(length = length(locations))\n  trother[]=alltran\n  trother=trother-sapply(tran,sum) # transition at other links = all transition - transition from local link - transition to local link\n  for(n in 1:length(locations)) trother[locout[[n]]]=trother[locout[[n]]]-tran[[n]]\n  pn=1-pnull+trother\n  \n  la2_tilde = lapply(1:length(locations), function(n) upd_forward(la1[[n]],pinc[n],pout[n],pn[n],pdec[n],max.person[n]+1) )\n  \n  lg2=lapply(1:length(locations), function(n) la2_tilde[[n]]*lb2[[n]] )\n  K=sapply(lg2, sum )\n  la2_tilde=lapply(1:length(locations), function(n) la2_tilde[[n]]/K[n])\n  \n  # expected events\n  if (TRUE) {\n    exp_null = lapply(1:length(locations), function(n)  rateout[[n]] * array(fra.eq[n],dim=length(locout[[n]]))  )\n    event_normalization = sum(unlist(tran))+(1-sum(unlist(exp_null)))\n    exp_event = Map('/',tran,event_normalization)\n  }\n  \n  list(la2_tilde=la2_tilde,exp_event=exp_event)\n}\n\nrate_in_out_f <- function(rate_slice,action_list) {\n  rateout = lapply(rate_slice,function(x) x[-1])\n  \n  ratein = list()\n  for(t1 in 1:length(locations)) {\n    tmp1 = locin[[t1]]\n    for(t2 in 1:length(tmp1)) {\n      tmp1[t2] = rate_slice[[tmp1[t2]]][which(action_list[[tmp1[t2]]]==t1)]\n    }\n    ratein[[t1]] = tmp1\n  }\n  list(rateout=rateout,ratein=ratein)\n}\n\nforward2 = function(start.time, end.time, la, lb, action_list, max.person,policy_network,learnable){\n  \n  exp_events_list = list()\n  mini_timesteps = array(0,dim=1441)\n  new.t = c()\n  length.la = length(la)\n  length(exp_events_list) = 1441\n  cv_old = array(0,dim=c((end.time-start.time),length(learnable)))\n  \n  for(i in start.time:(end.time-1)){\n    la1=la[[i]]\n    lb1=lb[[i]]\n    lb2=lb[[i+1]]\n    \n    lg=matrix(c(sapply(input_state,function(n) {\n      gamma=la1[[n]]*lb1[[n]]\n      gamma=gamma/sum(gamma)\n      sum(gamma* (0:(length(gamma)-1)))\n      }),i),nrow = 1)\n    lg = lg/state_scale\n    \n    action = policy_network %>% predict(lg)\n    action = action/action_scale\n    \n    cv_old[i-start.time+1,] = action\n    \n    rate_slice = action2rate(action,max.person,la1,lb1)\n    \n    rate = rate_in_out_f(rate_slice,action_list)\n    ratein_original=rate$ratein \n    rateout_original=rate$rateout\n    \n    m.eq=numeric(length = length(locations))\n    m.eq=sapply(1:length(locations),function(n) sum(la1[[n]]*lb2[[n]]))\n    m.eq[m.eq==0]=1e-20\n    \n    m.eq.x=numeric(length = length(locations))\n    m.eq.x=sapply(1:length(locations),function(n) sum(0:max.person[n]*la1[[n]]*lb2[[n]]))\n    \n    pout=sapply(rateout_original, sum)\n    pnull= sum(pout*m.eq.x/m.eq) - pout*m.eq.x/m.eq\n    r_nnn=max.person*pout+pnull\n    nnn = max(ceiling(r_nnn))\n    \n    pout=pout/nnn\n    pnull=pnull/nnn\n    ratein=lapply(1:length(ratein_original), function(n) ratein_original[[n]]/nnn)\n    rateout=lapply(1:length(rateout_original), function(n) rateout_original[[n]]/nnn)\n    \n    #print(\"--------------------------------nnn---------\")\n    #print(nnn)\n    mini_timesteps[i] = nnn\n    if(nnn>1) new.t=c(new.t,i+1:(nnn-1)/nnn)\n    \n    cv = sapply(1:length(locations), function(n) {\n      array(0,dim=length(action_list[[n]])-1)\n    })\n    \n    for (k in 1:nnn){\n      t1 = i+(k-1)/nnn; t2 = i+k/nnn;\n      lb2=getSlice(lb,t2);\n      \n      if(k!=nnn) {\n        tran=transition_forward_fra(la1,lb2,ratein, locin, rateout, locout, pout, pnull)\n        la2=tran$la2_tilde\n        \n        if(length(attr(la,'t'))==length.la){la = alloc(la); length.la = length(la)}\n        if(min(abs(t2-attr(la,'t')))<1e-6) {\n          idx = which.min(abs(t2-attr(la,'t')))\n        } else {\n          attr(la,'t') = c(attr(la,'t'),t2);\n          idx = length(attr(la,'t'))\n        }\n        la[[idx]] = la2\n      } else {\n        tran=transition_forward_fra(la1,lb2,ratein, locin, rateout, locout, pout, pnull)\n        la2=tran$la2_tilde\n        la[[i+1]]=la2\n      }\n      \n      exp_events = tran$exp_event\n      cv_t = sapply(1:length(locations), function(n) {\n        tmp_vector = exp_events[[n]]/sum(la1[[n]]*lb1[[n]]*0:max.person[n])\n        tmp_vector[is.na(tmp_vector)] = 0\n        tmp_vector\n      })\n      cv = Map('+',cv,cv_t)\n      \n      la1=la2\n      lb1=lb2\n    } # k\n    \n    exp_events_list[[i+1]]=cv\n  }\n  \n  #length.t = c(start.time:end.time,new.t)\n  new.t=c(1:1441,new.t)\n  la = unclass(la)[match(new.t,attr(la,'t'))]; attr(la,'t') = new.t;  attr(la,'c')=\"a\"\n  \n  list(la=la, new.t=new.t, mini_timesteps=mini_timesteps, exp_events_list=exp_events_list, cv_old=cv_old)\n}\n\n# backward\n#######################################################################################################\n\nupd_backward_test<-function(v1,inc,out,pn,dec,len){\n  v2=v1*pn - 0:(len-1)*v1*out\n  v2[1:(len-1)]=v2[1:(len-1)]+v1[2:len]*inc\n  v2[2:len]=v2[2:len]+1:(len-1)*v1[1:(len-1)]*dec\n  v2=v2\n  v2\n}\n\nupd_backward<-function(v1,inc,out,pn,dec,len,lb_tm_n,lb_factor){\n  v2=v1*pn - 0:(len-1)*v1*out\n  v2[1:(len-1)]=v2[1:(len-1)]+v1[2:len]*inc\n  v2[2:len]=v2[2:len]+1:(len-1)*v1[1:(len-1)]*dec\n  v2=v2+lb_tm_n*lb_factor\n  v2\n}\n\ntransition_backward_fra<-function(la1,lb2,ratein, locin, rateout, locout, pout, pnull,lb_tm,lb_factor){\n  m.inc=sapply(1:length(locations),function(n) sum( la1[[n]][1:max.person[n]] * lb2[[n]][2:(max.person[n]+1)] ) )\n  m.eq=sapply(1:length(locations),function(n) sum(la1[[n]]*lb2[[n]]))\n  m.eq[m.eq==0]=1e-20\n  m.dec=sapply(1:length(locations),function(n) sum( 1:max.person[n] * la1[[n]][2:(max.person[n]+1)] * lb2[[n]][1:max.person[n]] ))\n  \n  fra.inc=m.inc/m.eq\n  fra.dec=m.dec/m.eq\n  pinc=sapply(1:length(locations),function(n) sum(  ratein[[n]] * fra.dec[locin[[n]] ] )) \n  pdec= sapply(1:length(locations),function(n)  sum(  rateout[[n]] * fra.inc[locout[[n]]  ] ) )\n  \n  #for each link, calculate the prob of transition at all other links \n  tran=lapply(1:length(locations), function(n)  rateout[[n]] * fra.dec[n] * fra.inc[locout[[n]]  ] )\n  alltran=sum(unlist(tran))\n  trother=numeric(length = length(locations))\n  trother[]=alltran\n  trother=trother-sapply(tran,sum) # transition at other links = all transition - transition from local link - transition to local link\n  for(n in 1:length(locations)) trother[locout[[n]]]=trother[locout[[n]]]-tran[[n]]\n  pn=1-pnull+trother\n  \n  lb1_tilde = lapply(1:length(locations), function(n) upd_backward(lb2[[n]],pinc[n],pout[n],pn[n],pdec[n],max.person[n]+1,lb_tm[[n]],lb_factor) )\n  \n  lg1=lapply(1:length(locations), function(n) la1[[n]]*lb1_tilde[[n]] )\n  K=sapply(lg1, sum )\n  lb1_tilde=lapply(1:length(locations), function(n) lb1_tilde[[n]]/K[n])\n  \n  list(lb1_tilde=lb1_tilde)\n}\n\nbackward2 = function(start.time, end.time, la, lb, action_list, max.person,policy_network,learnable){\n  new.t = c()\n  length.lb = length(lb)\n  \n  lb_tm = sapply(1:length(locations),function(n) array(1,dim=length(lb[[1]][[n]])))\n  lb_factor = 1/(end.time-start.time)/length(locations)\n\n  for(i in (end.time-1):start.time){\n    #print(i)\n    \n    la1=la[[i]]\n    lb1=lb[[i]]\n    lb2=lb[[i+1]]\n    \n    lg=matrix(c(sapply(input_state,function(n) {\n      gamma=la1[[n]]*lb1[[n]]\n      gamma=gamma/sum(gamma)\n      sum(gamma* (0:(length(gamma)-1)))\n    }),i),nrow = 1)\n    lg = lg/state_scale\n    \n    action = policy_network %>% predict(lg)\n    action = action/action_scale\n    \n    rate_slice = action2rate(action,max.person,la1,lb1)\n    \n    rate = rate_in_out_f(rate_slice,action_list)\n    ratein_original=rate$ratein \n    rateout_original=rate$rateout\n    \n    m.eq=numeric(length = length(locations))\n    m.eq=sapply(1:length(locations),function(n) sum(la1[[n]]*lb2[[n]]))\n    m.eq[m.eq==0]=1e-20\n    \n    m.eq.x=numeric(length = length(locations))\n    m.eq.x=sapply(1:length(locations),function(n) sum(0:max.person[n]*la1[[n]]*lb2[[n]]))\n    \n    pout=sapply(rateout_original, sum)\n    pnull= sum(pout*m.eq.x/m.eq) - pout*m.eq.x/m.eq\n    r_nnn=max.person*pout+pnull\n    nnn = max(ceiling(r_nnn))\n    \n    pout=pout/nnn\n    pnull=pnull/nnn\n    ratein=lapply(1:length(ratein_original), function(n) ratein_original[[n]]/nnn)\n    rateout=lapply(1:length(rateout_original), function(n) rateout_original[[n]]/nnn)\n    \n    #print(\"--------------------------------nnn---------\")\n    #print(nnn)\n    \n    if(nnn>1) new.t=c(new.t,i+(nnn-1):1/nnn)\n    \n    for (k in nnn:1){\n      t1 = i+(k-1)/nnn; t2 = i+k/nnn;\n      la1=getSlice(la,t1)\n      \n      if(k!=nnn) {\n        lb_tm = sapply(1:length(locations),function(n) array(0,dim=length(lb[[1]][[n]])))\n        tran=transition_backward_fra(la1,lb2,ratein, locin, rateout, locout, pout, pnull,lb_tm,lb_factor)        \n      } else {\n        lr = RewardFunction(max.person, i, locations, action_list, info_network)\n        lr = RewardNormalization(lr,R_min)\n        lb_tm = Map('+',lr,(length(locations)-1))\n        tran=transition_backward_fra(la1,lb2,ratein, locin, rateout, locout, pout, pnull,lb_tm,lb_factor)\n      }\n      \n      lb1=tran$lb1_tilde\n      lb2 = lb1\n      \n      if(k==1){\n        lb[[i]]=lb1\n      } else{\n        if(length(attr(lb,'t'))==length.lb){lb = alloc(lb); length.lb = length(lb)}\n        if(min(abs(t1-attr(lb,'t')))<1e-12) {\n          lb[[which.min(abs(t1-attr(lb,'t')))]] <- lb1\n        } else{\n          attr(lb,'t') = c(attr(lb,'t'),t1)\n          lb[[length(attr(lb,'t'))]]<-lb1\n        }\n      }\n      \n    } #k\n  }\n  \n  #length.t = c(start.time:end.time,new.t)\n  new.t=c(1:1441,rev(new.t) )\n  lb = unclass(lb)[match(new.t,attr(lb,'t'))]; attr(lb,'t') = new.t;  attr(lb,'c')=\"b\"\n  list(lb = lb)\n}\n\n# M step\n#######################################################################################################\n\nM_step = function(start.time, end.time, la, lb, theta, action_list, max.person, new.t, mini_timesteps,exp_events_list,policy_network,learnable,cv_old){\n  \n  train_x = array(0,dim=c(end.time-start.time,2+1))\n  train_y = array(0,dim=c(end.time-start.time,length(learnable)))\n  \n  for(i in start.time:(end.time-1)){\n    cv = exp_events_list[[i+1]]\n    la1 = la[[i]]\n    lb1 = lb[[i]]\n    train_x[i-start.time+1,1:2] = sapply(1:2,function(n) {\n      gamma=la1[[n]]*lb1[[n]]\n      gamma=gamma/sum(gamma)\n      sum(gamma* (0:(length(gamma)-1)))\n    })\n    train_x[i-start.time+1,3] = i\n    train_y[i-start.time+1,] = unlist(cv)[learnable]\n  }\n  \n  # line search\n  train_y = cv_old + 2*(train_y-cv_old)\n  train_y = pmax(train_y,0)\n  \n  list(train_x=train_x,train_y=train_y)\n}\n\n#######################################################################################################\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1526667258706.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1677865838",
    "id" : "944EB4CD",
    "lastKnownWriteTime" : 1526668356,
    "last_content_update" : 1526668356942,
    "path" : "~/Documents/R/RL_VI_2018_NIPS/VI_RL8_a=v_berlin/vi-functions.R",
    "project_path" : "vi-functions.R",
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}