{
    "collab_server" : "",
    "contents" : "\nsource('vi-functions.R')\n\n# policy network\nif (TRUE) {\n  # SynthTown\n  #policy_network = keras_model_sequential() %>% layer_dense(units=12, input_shape = c(length(input_state)+1), activation='relu') %>% layer_dense(units=24, input_shape = c(3), activation='relu') %>% layer_dense(units=length(learnable), activation='relu')%>% compile(optimizer = optimizer_adam(lr=.001),loss = 'mse')\n  # Berlin\n  policy_network = keras_model_sequential() %>% layer_dense(units=64, input_shape = c(length(input_state)+1), activation='relu') %>% layer_dense(units=64, input_shape = c(3), activation='relu') %>% layer_dense(units=length(learnable), activation='relu')%>% compile(optimizer = optimizer_adam(lr=.001),loss = 'mse')\n  \n  # pretrain\n  history <- policy_network %>% fit(x=train_x, y=train_y, epochs=1000, batch_size=100,verbose=0)\n  print(summary(history$metrics[[1]]))\n  \n  action = policy_network %>% predict(train_x)\n  \n  plot(rowSums(train_y[,1:9]),type='l')\n  lines(rowSums(action[,1:9]),type='l',col='red')\n  lines(train_y[,10],type='l')\n  lines(action[,10],type='l',col='blue')\n}\n\n# Main loop\n#######################################################################################################\nfor(iter_r in 1:RLITER){\n  cat(sprintf(\"iter_r: %d\\n\",iter_r))\n\n  la_E = la_original\n  lb_E = lb_original\n  xt_est = array(0,dim=c(nrow(obs),ncol(obs)))\n  train_x = c()\n  train_y = c()\n  \n  t0 = 1\n  start.time = 1\n  end.time = 1441\n  H = end.time - start.time\n  obs = loc.d\n  \n  # E-step\n  #######################################################################################################\n  if (TRUE) {\n    cat(sprintf(\"E-step\\n\"))\n    aaa = forward2(start.time, end.time, la_E, lb_E, action_list, max.person,policy_network,learnable)\n    la_E=aaa$la\n    \n    #estimate vehicle distribution\n    for(ii in 1:ncol(obs)){\n      xt_est[start.time:end.time,ii] = sapply(start.time:end.time, function(n){ \n        gamma=la_E[[n]][[ii]]\n        gamma=gamma/sum(gamma)\n        sum(gamma* (0:(length(gamma)-1))) })\n    }\n    \n    # compute rewards\n    lr_accum = 0\n    for(tt in start.time:(end.time-1)) {\n      lg_test = la_E[[tt]]\n      group_idx = 1\n      lr = RewardFunction(max.person, tt, locations, action_list, info_network)\n      lr = RewardNormalization(lr,R_min)\n      lr_l = unlist(sapply(c(1:length(locations)),function(n) sum(lr[[n]]*lg_test[[n]])))\n      lr_accum = lr_accum + sum(lr_l/length(locations)/(end.time-start.time))\n    }\n    lr_accum = log(lr_accum)\n  }\n  \n  # M-step\n  #######################################################################################################\n  if (TRUE) {\n    cat(sprintf(\"M-step\\n\"))\n    for(iter_v in 1:VIITER) {\n      bbb = backward2(start.time, end.time, la_E, lb_E, action_list, max.person,policy_network,learnable)\n      lb_E=bbb$lb \n      aaa = forward2(start.time, end.time, la_E, lb_E, action_list, max.person,policy_network,learnable)\n      la_E=aaa$la\n      exp_events_list=aaa$exp_events_list\n      cv_old=aaa$cv_old\n      new.t=aaa$new.t\n      mini_timesteps=aaa$mini_timesteps\n    }\n    \n    mstep = M_step(start.time, end.time, la_E, lb_E, theta, action_list, max.person, new.t, mini_timesteps,exp_events_list,policy_network,learnable,cv_old)\n    train_x = rbind(train_x,mstep$train_x)\n    train_y = rbind(train_y,mstep$train_y)\n  }\n  \n  if (TRUE) {\n    train_y = train_y*action_scale\n    train_x = train_x/state_scale\n    history <- policy_network %>% fit(x=train_x, y=train_y, epochs=2000, batch_size=100,verbose=0)\n    \n    print(summary(history$metrics[[1]]))\n    action = policy_network %>% predict(train_x)\n    plot(rowSums(train_y[,1:9]),type='l')\n    lines(rowSums(action[,1:9]),type='l',col='red')\n    lines(train_y[,10],type='l')\n    lines(action[,10],type='l',col='blue') \n  }\n  \n  # plot\n  #######################################################################################################\n  if (TRUE) {\n    if (iter_r == 1) {\n      xt_baseline = xt_est\n    }\n    \n    #plot vechile distribution\n    layout(matrix(1:ncol(obs),ncol=1), heights=pmax(apply(obs,2,max),apply(loc.d,2,max))+10)\n    par(mar=c(0,0,0,0),oma=c(5,2,0,1)+.1)\n    for(ii in 1:ncol(obs)){\n      plot(xt_baseline[,ii],type='l',col='black',xaxt='n',xlab='',ylab=colnames(obs)[ii],ylim = c(0,max(loc.d[,ii])+1))\n      lines(xt_est[,ii],col=\"red\",lty=1)\n      if(ii==1) text(1300,10,paste(iter_r, \" f lg\" ),col = 'red',lwd=3)\n      abline(v=c(540,1020),col='gray')\n    }\n    axis(side=1)\n    \n    # compute rewards\n    lr_history = lappend(lr_history,lr_accum)\n    cat(sprintf(\"System log expected future rewards at %d: %f\\n\",t0,lr_accum))\n  }\n  \n  # save data\n  if (TRUE) {\n    xt_est_vi = xt_est\n    save(xt_est_vi,file='result/xt_est_vi.RData')\n    save(lr_history,file='result/lr_history.RData')\n  }\n}\n\n\n\n\n\n\n\n\n\n",
    "created" : 1526667259382.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3337702242",
    "id" : "DD2968CF",
    "lastKnownWriteTime" : 1526668362,
    "last_content_update" : 1526668362353,
    "path" : "~/Documents/R/RL_VI_2018_NIPS/VI_RL8_a=v_berlin/vi-main.R",
    "project_path" : "vi-main.R",
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}